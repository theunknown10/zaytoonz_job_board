import Turndown from 'turndown';
import { LlamaModel } from 'node-llama-cpp';
import { generateLlamaCompletions, generateAISDKCompletions, streamAISDKCompletions, generateAISDKCode, } from './models.js';
import cleanup from './cleanup.js';
export default class LLMScraper {
    client;
    constructor(client) {
        this.client = client;
        this.client = client;
    }
    // Pre-process a page
    async preprocess(page, options = { format: 'html' }) {
        const url = page.url();
        let content;
        if (options.format === 'html') {
            content = await page.content();
        }
        if (options.format === 'markdown') {
            const body = await page.innerHTML('body');
            content = new Turndown().turndown(body);
        }
        if (options.format === 'text') {
            const readable = await page.evaluate(async () => {
                const readability = await import(
                // @ts-ignore
                'https://cdn.skypack.dev/@mozilla/readability');
                return new readability.Readability(document).parse();
            });
            content = `Page Title: ${readable.title}\n${readable.textContent}`;
        }
        if (options.format === 'cleanup') {
            await page.evaluate(cleanup);
            content = await page.content();
        }
        if (options.format === 'image') {
            const image = await page.screenshot({ fullPage: options.fullPage });
            content = image.toString('base64');
        }
        if (options.format === 'custom') {
            if (!options.formatFunction ||
                typeof options.formatFunction !== 'function') {
                throw new Error('customPreprocessor must be provided in custom mode');
            }
            content = await options.formatFunction(page);
        }
        return {
            url,
            content,
            format: options.format,
        };
    }
    // Generate completion using AI SDK
    async generateCompletions(page, schema, options) {
        switch (this.client.constructor) {
            default:
                return generateAISDKCompletions(this.client, page, schema, options);
            case LlamaModel:
                return generateLlamaCompletions(this.client, page, schema, options);
        }
    }
    // Stream completions using AI SDK
    async streamCompletions(page, schema, options) {
        switch (this.client.constructor) {
            default:
                return streamAISDKCompletions(this.client, page, schema, options);
            case LlamaModel:
                throw new Error('Streaming not supported with GGUF models');
        }
    }
    async generateCode(page, schema, options) {
        switch (this.client.constructor) {
            default:
                return generateAISDKCode(this.client, page, schema, options);
            case LlamaModel:
                throw new Error('Code-generation not supported with GGUF models');
        }
    }
    // Pre-process the page and generate completion
    async run(page, schema, options) {
        const preprocessed = await this.preprocess(page, options);
        return this.generateCompletions(preprocessed, schema, options);
    }
    // Pre-process the page and stream completion
    async stream(page, schema, options) {
        const preprocessed = await this.preprocess(page, options);
        return this.streamCompletions(preprocessed, schema, options);
    }
    // Pre-process the page and generate code
    async generate(page, schema, options) {
        const preprocessed = await this.preprocess(page, {
            ...options,
            format: 'cleanup',
        });
        return this.generateCode(preprocessed, schema, options);
    }
}
